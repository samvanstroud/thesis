\chapter{Tracking and Flavour Tagging at High-\texorpdfstring{\pt}{pt}}\label{chap:tracking}

The various flavour tagging algorithms introduced in \cref{sec:btagging_algs} work by identifying the unique signatures of heavy flavour (HF) jets ($b$- and \cjets).
Ultimately, the tagging algorithms use input information about the reconstructed jet and its associated tracks.
Successful \btagging therefore relies on the efficient and accurate track reconstruction, especially for tracks corresponding to the products of HF decays.
In this chapter the challenges facing track reconstruction and flavour tagging at \highpt are discussed.


The chapter is structured as follows.
In \cref{sec:track_classifier_datasets} an introduction to the datasets used in this thesis is given.
A summary of the challenges facing tracking and \btagging at high transverse momentum is provided in \cref{sec:b_had_reco_chall}.
Some preliminary investigations into improving tracking in the \highpt regime are investigated in \cref{sec:b_track_reco_improvements}.
Finally, in \cref{sec:trk_btag_conclusion} the conclusions of the chapter are given.


\section{Datasets}\label{sec:track_classifier_datasets}

This thesis makes extensive use of two simulated datasets which are described in this section.
The datasets are made up of simulated SM \ttbar and BSM \Zprime events initiated by proton-proton collisions at a center of mass energy $\sqrt{s} = \SI{13}{\TeV}$.
The \Zprime sample is constructed in such a manner that it has a relatively flat jet \pt spectrum up to \SI{5}{\TeV} and decays democratically to equal numbers of \bcl jets.
The generation of the simulated event samples includes the effect of multiple proton-proton interactions per bunch crossing with an average pile-up of $\langle \mu \rangle = 40$, which includes the effect on the detector response due to interactions from bunch crossings before or after the one containing the hard interaction.

\newcommand{\hdampFootnote}{%
The $h_\text{damp}$ parameter is used to limit the effect of resummed higher order corrections. It is used to suppress the transverse momentum of the radiation which the \ttbar system recoils against.}
%effectively regulating the %
%is a resummation damping factor and one of the parameters that controls the matching of \textsc{Powheg} matrix elements to the parton shower and thus effectively regulates the %high-$p_T$ radiation against which the \ttbar system recoils.}

The \ttbar events are generated using the \textsc{PowhegBox} \textsc{v2} generator  \cite{powheg2004, powheg2007, powheg2007_2, powheg2010} at next-to-leading order with the NNPDF3.0NLO \cite{Ball:2014uwa} set of parton distribution functions (PDFs). The $h_\text{damp}$ parameter\footnote{\hdampFootnote} is set to 1.5 times the mass of the top-quark ($m_\text{top}$)~\cite{ATL-PHYS-PUB-2016-020}, with $m_\text{top} = \SI{172.5}{\GeV}$. 
The events are interfaced to \textsc{Pythia}~8.230~\cite{Sjostrand:2014zea} to model the parton shower, hadronisation, and underlying event, with parameters set according to the A14 tune \cite{ATL-PHYS-PUB-2014-021} and using the NNPDF2.3LO set of PDFs \cite{Ball:2012cx}. 
\Zprime events are generated with \textsc{Pythia} 8.2.12 with the same tune and PDF set.
The decays of \bchadrons are performed by \textsc{EvtGen} v1.6.0 \cite{Lange:2001uf}.
Particles are passed through the ATLAS detector simulation \cite{SOFT-2010-01} based on GEANT4 \cite{Agostinelli:2002hh}.

%Additionally, all jets are required not to overlap with a generator-level electron or muon from \Wboson boson decays.
Jets are required to have a pseudorapidity $|\eta| < 2.5$ and $\pt > \SI{20}{\GeV}$.
Additionally, a standard selection using the JVT tagger (see \cref{sec:jet_reco}) at the tight working point is applied to jets with $\pt < \SI{60}{\GeV}$ and $|\eta| < 2.4$ in order to suppress pile-up contamination \cite{ATLAS-CONF-2014-018}.




\section{\texorpdfstring{\bhadron}{b-hadron} Reconstruction Challenges}
\label{sec:b_had_reco_chall}

%This section some of the difficulties in the accurate reconstruction of the  associated reconstruction difficulties in \cref{sec:b_had_reco_chall}.
As discussed in \cref{sec:b_decay_topology}, a necessary requirement for successful \btagging is the efficient and accurate reconstruction of the charged particle trajectories in the jet.
For high \pT jets (\pT $> 250$ GeV) this task becomes difficult due to a difficulties in the accurate reconstruction of tracks, as described below.

As the \bjet energy increases, the multiplicity of the fragmentation products inside the jet increases, while the multiplicity of the products of the weak decay remains fixed.
The ``signal'' tracks (those from the weak decay of the \bhadron) therefore become significantly outnumbered.
Both fragmentation and \bhadron weak decay products also become increasingly collimated as their inherited transverse momentum increases.
This is compounded by the increased decay length of \bhadrons (and \chadrons) at \highpt, which means that the decay products have less of an opportunity to diverge before reaching the first tracking layers of the detector (shown in \cref{fig:high_pt_b_decay}).
If the weak decay of the \bhadron takes place close enough to a detector layer, or if the particles are otherwise sufficiently collimated, charge deposits left by nearby particles may not be resolved individually, instead being reconstructed as merged clusters \cite{PERF-2015-08}.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{chapters/3.tracking/figs/high_pt_b_decay.pdf}
  \caption{
    At lower \pt (left) the decay length of the \bhadron is on average reduced, and the decay tracks are less collimated.
    At higher \pt (right) the \bhadron decay length increases and the resulting decay tracks are more collimated and have less distance over which to diverge before reaching detector elements.
    As a result, the ID may be unable to resolve charged depositions from different particles, resulting merged clusters.
  }
  \label{fig:high_pt_b_decay}
\end{figure}

As discussed in \cref{sec:track_reco}, merged clusters are generally rare, and so shared hits generally predict bad tracks and are correspondingly penalised during track reconstruction.
However, in the core of high \pT \bjets the density of particles is high enough that the probability of cluster merging increases dramatically.
Successful reconstruction of such tracks requires the presence of shared hits to be effectively dealt with, but in the standard reconstruction the presence of these can instead impair the successfully reconstruction of the track.
Furthermore, HF decays may also take place inside the tracking detectors themselves, which at best leads to missing measurements on the most sensitive detector layers, and at worst can lead to wrong inner layer hits being added to displaced tracks, since the reconstruction process penalises tracks without inner layer hits.

%Together, these two effects lead to a high density of charged particles in the jet core, which, given the finite resolution of the detector, makes reconstruction difficult.




\begin{comment}
%
\begin{figure}[!htbp]
    \centering
    %\includegraphics[width=\textwidth]{res/figs/results/tracking/b-reco-efficiency.png}
    \vspace{0.05em}
    \caption{Track reconstruction efficiency from \bhadron decay products for baseline ATLAS tracking (black), Bcut+Refit procedures applied (green), pseudo-tracking (blue), and for tracking where the ambiguity solver has been manually removed (orange).}
    %The relatively high reconstruction efficiency at the stage of the track candidates (i.e. before ambiguity solving) indicates that the efficiency loss is driven by the ambiguity solver.
    \label{fig:reconstruction efficiency from B}
\end{figure}

\begin{figure}[!htbp]
    \centering
    %\includegraphics[width=\textwidth]{res/figs/results/tracking/po_nHitsOnPix_From_B_DL.pdf}
    \vspace{0.05em}
    \caption{The total number of pixel hits on tracks from \bhadron decays as a function of the production radius of the decay product. An excess of hits is assigned to the standard tracks in comparison to the ideal pseudotracks.}
    \label{fig:total hits on pix from b}
    \label{fig:misc}
\end{figure}
%
\end{comment}


The above effects create two distinct but related  problems for \btagging.
The first is a drop in track reconstruction efficiency.
%As mentioned, tracks originating from high energy \bhadron decay products can have a high rate of shared hits due to the number of particles present in a high \pT \bjet and their relative collimation.
%Additionally, tracks may be missing hits on the inner layers of the detector in the case of displaced decays.
The presence of shared and missing hits reduces a track's score in the ambiguity solver meaning that higher ranking, but potentially less accurate, track candidates are processed first and take ownership of the hits.
This can make it difficult for otherwise reasonable \bhadron decay tracks to meet the ambiguity solver's stringent track quality requirements, leading to their rejection at this stage and an overall decrease in the \bhadron decay track reconstruction efficiency.
As shown in \cref{fig:b_track_eff}, this can result in a large drop in reconstruction efficiency for \bhadron decay products of up to \pct{50} for at $\pt > \SI{2}{\TeV}$.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.6\textwidth]{chapters/3.tracking/figs/b_track_reco_eff.png}
  \caption{
    \bhadron decay track reconstruction efficiency as a function of truth \bhadron \pt \cite{2022DonalTrackReco}.
    Nominal track reconstruction is shown in black, while the track reconstruction efficiency for track candidates (i.e. the pre-ambiguity solver efficiency) is shown in green.
    For \highpt \bhadrons, the ambiguity solver is overly aggressive in its removal of \bhadron decay tracks.
    Suggestions for the improvement of the track reconstruction efficiency in this regime by the loosening of cuts in the ambiguity solver are shown in blue and red.
  }
  \label{fig:b_track_eff}
\end{figure}

%As a result, many B decay tracks are rejected in the ambiguity solving stage, leading to a severe drop in tracking reconstruction efficiency. This is shown by the severe decrease in reconstruction efficiency visible when comparing baseline tracking with the ideal pseudotracks in \cref{fig:reconstruction efficiency from B}. This situation presents a problem: relaxing cuts on shared hits significantly degrades the ambiguity solver's power to reject bad tracks. However for \bhadron decay tracks it seems these same restrictions on shared hits are seriously impairing the reconstruction efficiency of good tracks. 

The second part of the problem is that, due to the high multiplicity of clusters available for assignment in the vicinity of the typical \highpt \bhadron decay track, and also given the strong positive bias of the ambiguity solver towards those tracks with pixel measurements in each layer (especially the innermost IBL measurement), many \bhadron decay tracks are assigned incorrect inner layer hits.
This is only a problem for those decay products which were produced within the pixel detector as a result of a significantly displaced \bhadron decay, and so do not have a correct hit available for assignment.
\cref{fig:pseudo_pix_hits} shows the number of hits as a function of the reconstructed track \pt for fragmentation tracks and tracks from the weak decay of the \bhadron.
The baseline tracks represent the standard reconstruction setup, while the pseudotracks represent the ideal tracking setup as outlined in \cref{sec:pseudotracks}.
Hit multiplicities on the pseudotracks decrease at high \pt due to the flight of the \bhadron before its decay. 
The baseline tracks have more hits than the pseudotracks, indicating that they are being incorrectly assigned additional hits on the inner layers of the detector.

\begin{figure}[!htbp]
  \centering
  \begin{subfigure}{.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{chapters/3.tracking/figs/overlay_po_nHitsOnIBL_From_B_pT.pdf}
    %\caption{}
    %\label{fig:n hits on ibl}
  \end{subfigure}%
  \begin{subfigure}{.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{chapters/3.tracking/figs/overlay_po_nHitsOnPix_From_B_pT.pdf}
    %\caption{}
    %\label{fig:n hits on pix}
  \end{subfigure}
  \caption{
    Hit multiplicities on the IBL (left) and the pixel layers (right) as a function of the \pT of the reconstructed track for tracks in jets in a \Zprime sample at \come{13}.
    Tracks from the weak decay of the \bhadron are shown in red, while fragmentation tracks (which are prompt) are in blue.
    Baseline tracks are those produced in the standard reconstruction described in \cref{sec:track_reco}, while pseudotracks represent the ideal performance of the ATLAS detector and are described in \cref{sec:pseudotracks}.
  }
  \label{fig:pseudo_pix_hits}
\end{figure}

These incorrect hits may skew the parameters of the track, which can in turn lower the performance of the downstream \btagging algorithms.
In particular, \btagging algorithms rely heavily on the transverse impact parameter significance \dzerosig of the track (see \cref{sec:track_parameterisation}).
The quality of this measurement is expected to be adversely affected by wrong inner-layer hits on the track.
Furthermore, multiple tracks sharing an incorrect hit can lead to the creation of spurious secondary vertices, which can cause further problems for the \btagging algorithms.

The combination of the effects described makes reconstructing tracks in the core of high \pT \bjets particularly challenging.
The reduced reconstruction efficiency of \bhadron decay tracks and incorrectly assigned hits is thought to be the primary cause of the observed drop in \btagging efficiency at high energies, however further study is required to determine which effect may dominate.



\section{High \texorpdfstring{\pT}{pT} \texorpdfstring{\bhadron}{b-hadron} Tracking Improvements}\label{sec:b_track_reco_improvements}

In \cref{sec:sharedhits} pseudotracks, a key tool for studying the ideal tracking performance of the \ATLAS detector, are used to study the shared hit requirements on tracks in the dense cores of \highpt \bjets.
Meanwhile \cref{sec:gx2f_opt} details a study which investigated modifying the global track fitter to improve reconstruction performance in this regime.
%Not detailed here is an investigation into the bcut + refit method. Shown not to be promising, as, even though an improvement in the \bhadron decay track efficiency was observed, the corresponding increase in the fake track was shown to be unacceptable.


\subsection{Shared Hits}\label{sec:sharedhits}

The ambiguity solver is not run for pseudotracks.
However, if the standard track collection is produced alongside the pseudotracks, then cluster splitting neural networks will be run for the standard tracks, and the resulting classification of clusters will be propagated to hits on pseudotracks.
This quirk allows one to study the inefficiencies of the cluster splitting process, and relatedly to determine whether shared hit cuts in the ambiguity solver are too loose or too tight.
The fraction of hits that are shared for the IBL and the B-layer is shown in \cref{fig:shared_hits_pseudo}.
The shared hits on pseudotracks represent correctly assigned hits from merged clusters that were not able to be classified as split by the cluster splitting neural networks.
As such, these represent the number of shared hits the ambiguity solver should aim to allow given the current performance of the cluster splitting algorithm.
For shared hits on the IBL for particles produced before the IBL, the baseline selection appears to be successful in disallowing excessive numbers of shared hits.
However, the ambiguity solver fails to limit shared hits for those particles produced after the IBL, reflecting the previously discussed problem of displaced tracks picking up incorrect hits.
Meanwhile, it is clear that for the B-layer, the ambiguity solver is being overly aggressive in its rejection of shared hits.

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=\textwidth]{chapters/3.tracking/figs/po_nSharedOnIBL_From_B_DL.pdf}
      %\caption{}
      %\label{fig:shared_hits_pseudo on IBL}
    \end{subfigure}%
    \begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=\textwidth]{chapters/3.tracking/figs/po_nSharedOnBL_From_B_DL.pdf}
      %\caption{}
      %\label{fig:shared_hits_pseudo on B}
    \end{subfigure}
    \caption{
      The fraction of IBL (left) and B-layer (right) hits which are shared on \bhadron decay tracks as a function of the production radius of the \bhadron decay product for tracks in jets in a \Zprime sample at \come{13}. 
      Pseudotracks represent the ideal performance given the ATLAS detector, see \cref{sec:pseudotracks}.
    }
    \label{fig:shared_hits_pseudo}
\end{figure}



\begin{comment}

\subsection{Looser Track Cuts \& Track Refit Procedure}\label{sec:bcut and refit}
A solution for the problem of wrong inner-layer hits on $B$ tracks had previously been developed. This solution selects tracks which pass a $b$-jet Region of Interest (ROI) selection, and then removes the innermost hits on these tracks based on the result of a ``refit'' procedure. The refit procedure runs as follows. Each track is refitted without the innermost hit, and if there is a significant improvement in the fit quality (the $\chi^2$ of the track fit divided by the number of degrees of freedom on the track $n$), the innermost hit is rejected and the new track is replaces the old. If the fit quality does not improve by a certain amount, the initial track is kept. This procedure is recursively applied. The $b$-jet ROI selection selects tracks that are matched within $dR < 0.14$ ($|\eta| < 0.1$, $|\phi| < 0.1$) of a CaloCluster with $E_T > 150$ GeV. The track itself must also pass a transverse momentum cut with \pT$>15$ GeV. The refit procedure was previously shown to lead to a reduction in the rate of wrongly assigned IBL hits on $B$ decay tracks (see \cref{fig:refit optimisation results sub2}). However, this apparent improvement did not lead to an increase in $b$-tagging performance. It was found that the refit procedure also removed unacceptable numbers of good hits, degrading the quality of un-problematic tracks, shown in \cref{fig:refit optimisation results sub1}. This is likely the cause of the underwhelming $b$-tagging performance improvement. 

The performance of both the ROI, and the hit removal using track fit information, is examined, and an attempt at improving the performance of the refit procedure is made. Results are discussed in the following two sections.

\subsubsection{Region of Interest Optimisation}\label{sec:roi opt}

Selection cuts for the $b$-jet ROI were determined on a largely ad-hoc basis. An effort was made to systematically optimise the selection cuts. The decay tracks of \bhadrons are tightly collimated with the $B$ itself, with most decay products satisfying $dR(B, \text{track}) < 0.02$, as shown in \cref{fig:B dR match sub1}. Meanwhile, calorimeter clusters relating to the \bhadrons are generally found within $dR < 0.05$ of the $B$ \cref{fig:B dR match sub2}. In total, then, $B$ decay tracks will usually be found within $dR<0.07$ of the relevant calorimeter cluster, which suggests that the current $dR<0.14$ is loose by a factor of two. Similar analysis of cluster and track energy distributions found that the related cuts were also loose, and so they were modified from $E_T > 150$ GeV to $E_T > 300$ GeV, and from $p_T > 15$ GeV to $p_T > 30$ GeV. 

Additionally examined in the course of this work was the fake rate of the $b$-jet ROI. The distributions in \cref{fig:cluster purity in pt} demonstrate that most of clusters passing the $E_T > 150$ GeV selection were unable to be matched to a nearby \bhadron using truth information. Clusters that pass the selection but do not correspond to energy depositions from \bhadrons lead to fake ROIs. As a consequence of these distributions, tracks selected by the ROI are largely impure in the desired \bhadron tracks.

The modified ROI was used to re-run the refit procedure. A comparison of ``standard'' and ``optimised'' (using the optimised $b$-jet ROI) refit procedures is found in \cref{fig:refit optimisation results}. These results show that whilst tighter selection cuts did lead to a recovery of some good hits (\cref{fig:refit optimisation results sub1}), performance with respect to the baseline is still significantly degraded. 

%
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{.4\textwidth}
      \centering
      %\includegraphics[width=\textwidth]{res/figs/results/tracking/Bhad-track-dR-3.png}
      \caption{}
      \label{fig:B dR match sub1}
    \end{subfigure}%
    \begin{subfigure}{.4\textwidth}
      \centering
      %\includegraphics[width=\textwidth]{res/figs/results/tracking/Bhad-CC-dR-3.png}
      \caption{}
      \label{fig:B dR match sub2}
    \end{subfigure}
    \caption{Distributions of angular distance $dR$ between \bhadrons and their weak decays and other fragmentation tracks (\cref{fig:B dR match sub1}), and the distribution of angular distance $dR$ between \bhadrons and the calorimeter clusters in the hadronic calorimeter (\cref{fig:B dR match sub2}). In \cref{fig:B dR match sub1}, the tracks from the weak decay of the $B$ are significantly more collimated to the $B$ than the other fragmentation tracks.}
    \label{fig:B dR match}
\end{figure}
%

\subsubsection{Fit Quality as a Discriminant for Wrong Hits}
As mentioned, tracks selected by the ROI are refitted without their innermost hit, and, if an improvement in fit quality is observed, the hit is rejected. In order to test the effectiveness of this procedure, a dataset of two sets of tracks was produced. The first set contained unmodified baseline-reconstructed tracks. The second contained the same tracks as the first, but modifications made during reconstruction removed the innermost hit on each track. Then, using Monte Carlo (MC) truth information, a track-by-track fit quality comparison was made for tracks with good and wrong innermost hits. 

It is clear from the distributions in \cref{fig:refit chi2 dists} that the fit quality improvement (measured by fractional change in $\chi^2/n$ of the track before and after the innermost hit is removed) is not a discriminating variable for wrong hits, and indeed attempted optimisations of the of the refit procedure based on these distributions were found to be ineffectual. While wrong hits are likely to degrade the track fit, it is also true that any additional measurement, good or wrong, constrains the track, and therefore removal of that measurement will be likely to lead to an increase in the $\chi^2/n$ of the track. Removing hits in this way is therefore problematic.
%
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{.4\textwidth}
      \centering
      %\includegraphics[width=\textwidth]{res/figs/results/tracking/ROI-purity.png}
      \caption{}
      \label{fig:cluster purity in pt}
    \end{subfigure}%
    \begin{subfigure}{.4\textwidth}
      \centering
      %\includegraphics[width=\textwidth]{res/figs/results/tracking/hitdrop-chi2.png}
      \caption{}
      \label{fig:refit chi2 dists}
    \end{subfigure}
    \caption{The distribution of cluster transverse momentum, in \cref{fig:cluster purity in pt} for both clusters that were able (orange) and unable (blue) to be matched to a \bhadron using MC truth information. The normalisation shows that the majority of clusters are not matched to \bhadrons, resulting in fake ROIs. In \cref{fig:refit chi2 dists}, the fractional improvement in track fit quality ($\chi^2/n$) is shown for all track (blue), tracks with good IBL hits (green), and tracks with wrong IBL hits (orange). The distributions are overlapping, suggesting that the $\chi^2/n$ improvement is not a good discriminator of good and wrong hits.}
    \label{fig:cluster chi2 info}
\end{figure}
%

\subsubsection{Conclusion}
The work outlined in the two preceding sections has uncovered issues with both the $b$-jet ROI, and the methodology of identification and removal of wrong hits on tracks inside a given ROI. Attempts were made to optimise the selection cuts of the ROI, however the large background of energetic phenomena produced in collisions that are not \bhadron related means that the ROI is largely unsuccessful in selecting a pure sample of likely \bhadron candidates. An additional effort was made to improve the removal of wrong hits using other information in addition to the track fit improvement. Information such as the type and locations of its, and track $d_0$ were considered. While progress here was not insignificant, without substantial overhaul of the ROI to improve $B$ purity, the results were not strong enough to demonstrate any viable solutions that would successful target and then improve \bhadron decay tracks.
%
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{.4\textwidth}
      \centering
      %\includegraphics[width=\textwidth]{res/figs/results/tracking/po_nGoodHitsOnIBL_From_B_DL.pdf}
      \caption{}
      \label{fig:refit optimisation results sub1}
    \end{subfigure}%
    \begin{subfigure}{.4\textwidth}
      \centering
      %\includegraphics[width=\textwidth]{res/figs/results/tracking/po_nWrongHitsOnIBL_From_B_DL.pdf}
      \caption{}
      \label{fig:refit optimisation results sub2}
    \end{subfigure}
    \caption{Distributions of good (\cref{fig:refit optimisation results sub1}) and wrong (fig:refit optimisation results sub2) hit assignment rates on the IBL for tracks using baseline tracking (black), the original unmodified refit procedure (green), and the refit procedure with an optimise set of ROI selection cuts (blue). The IBL lies at a radius of 33 mm from the beam pipe. Hence, particles produced with a production radius greater than this cannot leave good hits on the IBL.}
    \label{fig:refit optimisation results}
\end{figure}
%
Alongside the refit procedure, a ``Bcut'' cut scheme was suggested in order to improve reconstruction performance. This consisted primarily of loosening the shared hit cuts in the ambiguity solver. While this did lead to a measurement increase in track reconstruction efficiency (see \cref{fig:b_track_eff}), it was determined that the corresponding increase in fake tracks (i.e. those tracks for which the majority of hits do not come from a single truth particle) was too large to justify the implementation of the ``Bcut'' scheme. In conclusion, then, a different approach is required to address the problems discussed.

\end{comment}


\subsection{Global \texorpdfstring{$\chi^2$}{chi2} Fitter Outlier Removal}\label{sec:gx2f_opt}

As part of the track fit an outlier removal procedure is run in which suspicious hits are identified and removed.
This section documents ongoing studies into improving hit-to-track assignment by using the Global $\chi^2$ Fitter (GX2F) to identify and prevent incorrect hits from being assigned to tracks during the track fit.
This is in contrast to a previously investigated approach \cite{AdorniBraccesiChiassi:2021irw} which attempted to identify and remove incorrect hits after the reconstruction of the track.

The GX2F code, as a relatively low-level component of track reconstruction, has not undergone significant modification for several years, and was originally only optimised in the context of prompt, isolated tracks.
Since then, a new tracking sub-detector, the IBL, was installed.
The motivation for looking at the GX2F is that this change may require the re-optimisation of the GX2F code, and in particular the outlier removal procedure.
Further motivation for this approach comes from the low rate of labelled outliers in baseline tracking, in contrast to the relatively higher rate of tracks with an incorrect IBL hit.
%For example, while approximately 15\% of \bhadron decay tracks have a wrong IBL hit (a value which only increases with the \pT of the \bhadron), less than 1\% of this tracks have had their IBL hit labelled and removed as an outlier.


\subsubsection{Implementation}
The outlier removal procedure for the pixel detector is described in this section. The hits on the track are looped over in order of increasing radial distance to the beam pipe. For each hit, errors $\sigma(m_i)$ on the measurement of the transverse and longitudinal coordinates are calculated. These errors are dependent on the sub-detector which recorded the measurement (some sub-detectors are more precise than others). Additionally, a residual displacement $r_i = m_i - x_i$ between the predicted position of the track $x_i$ (inclusive of the current measurement), and the position of the hit itself, $m_i$, is calculated. The pull $p_i$ on the track state due to the current measurement is calculated according to
%
\begin{equation}
    p_i = \frac{m_i - x_i}{\sqrt{\sigma(m_i)^2 - \sigma(x_i)^2}}
\end{equation}
%
This pull is computed for the transverse and longitudinal coordinates of the measurement, and the maximum of the two is selected and checked to see if it exceeds a certain selection threshold. If it does, the hit will be removed if the track also exceeds a threshold on the total $\chi^2/n$, where $n$ is the number of degrees of freedom on the track.
The results of varying the outlier selection and $\chi^2/n$ thresholds are described below.


\subsubsection{Selection Optimisation}

A systematic variation of the outlier selection and $\chi^2/n$ thresholds has been carried out.
Both thresholds were reduced in fixed step sizes of 0.25 for the outlier selection threshold and 1 for the $\chi^2/n$ threshold.
The results for the best performing selections are discussed below.
The value of the outlier selection threshold was reduced from 4 down to 1.75, a change which affects the silicon layers (the TRT has separate outlier removal logic).
Furthermore, a specific cut for the IBL was introduced, and after optimisation is set to 1.25.
The second threshold on the track $\chi^2/n$ was also reduced from 7 to 4.
Finally, instead of taking the maximum of the pulls in the longitudinal and transverse directions, a quadrature sum is taken of these two values and used.
This variation is labelled ``Mod GX2F'' and was found to improve performance.

The results are shown in \cref{fig:gx2f_opt_hits} and demonstrate a reduction in wrong hit assignment whist also improving slightly the rate at which good hits are assigned to tracks.
For a \SI{1}{\TeV} track, the rate to assign good hits to the track increases by approximately \pct{10}, while the rate to assign incorrect hits decreases by approximately \pct{16}.
The improvements are also observed when looking inclusively in all tracks, which avoids the need for a specific \bjet region-of-interest selection.

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=\textwidth]{chapters/3.tracking/figs/p_nGoodHitsIBL_pTB_From_B.pdf}
    \end{subfigure}%
    \begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=\textwidth]{chapters/3.tracking/figs/p_nWrongHitsIBL_pTB_From_B.pdf}
    \end{subfigure}
    \caption{
      The average number of good (left) and wrong (right) IBL hits as a function of \bhadron \pt for tracks using baseline tracking (black) and the modified version of the outlier removal procedure (red).
    }
    \label{fig:gx2f_opt_hits}
\end{figure}

An improvement, though modest, of all track parameter resolutions and pulls is observed.
The improvement for the transverse impact parameter pull is shown in \cref{fig:gx2f_opt_pulls}.
The results demonstrate an improvement in hit assignment, unchanged reconstruction efficiency, and modest improvement in track parameter resolutions and pulls. In addition, the truth match probability of tracks is unchanged, suggesting that there is no increase in fake track rates. The changes are expected to have a negligible impact on computational resources.

\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=\textwidth]{chapters/3.tracking/figs/h_recoTruthPull_d0_From_B.pdf}
      %\caption{}
      %\label{fig:d0 pull}
    \end{subfigure}%
    \begin{subfigure}{.48\textwidth}
      \centering
      \includegraphics[width=\textwidth]{chapters/3.tracking/figs/p_d0_pull_size_pTB_From_B.pdf}
      %\caption{}
      %\label{fig:z0 pull}
    \end{subfigure}
    \caption{(left) \bhadron decay track \dzero pulls (\dzero/\dzerosig) for baseline and modified GX2F tracks. (right) The absolute value of the \dzero pull as a function of the truth \bhadron transverse momentum.}
    \label{fig:gx2f_opt_pulls}
\end{figure}




\section{Conclusion}\label{sec:trk_btag_conclusion}

In this section, the difficulties facing efficient and accurate  track reconstruction, and hence performant \btagging, have been outlined.
The ambiguity solver, which attempts to clean or reject tracks which have an excess of shared hits, is shown to be overly aggressive in the rejection of \bhadron decay product track candidates.
The ambiguity solving process relies on a complicated pre-defined selection which has not been optimised for high transverse momentum \bhadron track reconstruction.
These conclusions have motivated further ongoing studies into the improvement of the track reconstruction in dense environments and the \highpt regime, such as those in \rcite{2022DonalTrackReco}.

An optimisation of the outlier removal process in the global $\chi^2$ fitter was carried out.
The results of the optimisation show that more aggressive removal of outlier hits can lead to fewer wrong hits being assigned to tracks, and improvements in the pulls of the track parameters.


\subsubsection{Future Work}
The studies were carried out in \rtwoone of the \ATLAS software, and need to be reproduced using the newer \rtwotwo to confirm the results against other changes in the baseline tracking configuration.
It is also necessary to study the impact of the improved outlier removal on the downstream \btagging algorithms.
Thanks to the all-in-one flavour tagging approach described in \cref{chap:gnn_tagger}, this will in future be easier to study.

As there are some known data-MC discrepancies, fine tuned optimisation such as the work presented here presents an opportunity to over-optimise the tracking algorithms on MC.
As such, further studies validating the improved outlier removal procedure on data are required.
